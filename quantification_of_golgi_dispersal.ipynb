{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM3BP9s1cG8zV1uQItHu1TD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RutikaSansaria17/Quantification-of-Golgi-Dispersal-and-Classification-Using-Machine-Learning-Models/blob/main/quantification_of_golgi_dispersal.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VJ_2NDooedS6"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "from skimage.io import imread, imshow\n",
        "from skimage import morphology, filters, io, segmentation, color\n",
        "from skimage.util import img_as_bool\n",
        "from skimage.feature import peak_local_max\n",
        "from scipy import ndimage as ndi\n",
        "from skimage.segmentation import watershed\n",
        "from skimage.measure import label, regionprops\n",
        "from scipy import ndimage\n",
        "import math\n",
        "from google.colab.patches import cv2_imshow\n",
        "import cv2\n",
        "from scipy.ndimage import distance_transform_edt\n",
        "from shapely.geometry import Polygon\n",
        "from skimage import measure\n",
        "from google.colab import files\n",
        "import pandas as pd\n",
        "import os\n",
        "import glob\n",
        "from pathlib import Path\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "from skimage.segmentation import watershed\n",
        "from skimage.feature import peak_local_max\n",
        "from skimage.morphology import skeletonize\n",
        "from scipy import ndimage as ndi"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "OH5inhwhej6L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/fish-quant/big-fish"
      ],
      "metadata": {
        "id": "vM9UzeNZemDA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import bigfish.segmentation as segment\n",
        "import bigfish.multistack as multistack\n",
        "import os\n",
        "\n",
        "import bigfish\n",
        "import bigfish.stack as stack\n",
        "import bigfish.plot as plot\n",
        "print(\"Big-FISH version: {0}\".format(bigfish.__version__))"
      ],
      "metadata": {
        "id": "N-pExtxTeoIS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def func (img,image_file):\n",
        "\n",
        "  physical_size =0.16049  #in um\n",
        "  # Print result\n",
        "  print(f\"Physical dimensions: {width_um:.2f} x {height_um:.2f} µm\")\n",
        "  physical_size = width_um / img.shape[0]    # assuming square pixels\n",
        "  print(f\"Physical size of pixel: {physical_size}\")\n",
        "  print(f\"Type of image: {img.dtype}\")\n",
        "\n",
        "  # Converting to gray-scale image\n",
        "  image = img[:,:,1]\n",
        "\n",
        "  # Denoising image\n",
        "  # bigfish.stack.remove_background_mean approximates the background of the image\n",
        "  # (the low frequency noise) by a large mean filter and substracts it from the original\n",
        "  # image. It is only available for 2D images.\n",
        "  image_background_mean = stack.remove_background_mean(image, kernel_shape=\"square\", kernel_size=50)\n",
        "  images = [image, image_background_mean]\n",
        "  titles = [\"Original image\", \"Removed mean background\"]\n",
        "  plot.plot_images(images, contrast=True, titles=titles)\n",
        "\n",
        "    # Apply Gaussian filter to theimage to correct the background\n",
        "  bkg_correction = filters.gaussian(image_background_mean, sigma=221)\n",
        "\n",
        "  # Divide the image by the background correction and normalize\n",
        "  corrected_cell = np.divide(image_background_mean, bkg_correction/bkg_correction.max())\n",
        "\n",
        "  # Apply Otsu threshold to the corrected image to segment foreground and background\n",
        "  threshold = filters.threshold_otsu(corrected_cell)\n",
        "  ostu_mask = corrected_cell > threshold\n",
        "\n",
        "  # Display the original and corrected images side by side\n",
        "  f4, a4 = plt.subplots(1,2, figsize=(20,10))\n",
        "  a41 = a4[0].imshow(image_background_mean, cmap='jet')\n",
        "  plt.colorbar(a41, ax=a4[0],fraction=0.05, pad=0.05)\n",
        "  a42 = a4[1].imshow(ostu_mask, cmap='jet')\n",
        "  plt.colorbar(a42, ax=a4[1],fraction=0.05, pad=0.05)\n",
        "\n",
        "  # Watershed segmentation\n",
        "\n",
        "  # Now we want to separate the objects in image\n",
        "  # Generate the markers as local maxima of the distance to the background\n",
        "  distance = ndi.distance_transform_edt(ostu_mask)\n",
        "  coords = peak_local_max(distance, footprint=np.ones((3, 3)), labels=ostu_mask)\n",
        "  mask = np.zeros(distance.shape, dtype=bool)\n",
        "  mask[tuple(coords.T)] = True\n",
        "  markers, _ = ndi.label(mask)\n",
        "  labels = watershed(-distance, markers, mask=ostu_mask)\n",
        "\n",
        "  # Find the bounding boxes of each labeled object\n",
        "  objects = ndi.find_objects(labels)\n",
        "\n",
        "  # Extract each object as a separate array using NumPy indexing\n",
        "  separated_objects = [ostu_mask[obj] for obj in objects]\n",
        "\n",
        "  fig, axes = plt.subplots(ncols=4, figsize=(9, 3), sharex=True, sharey=True)\n",
        "  ax = axes.ravel()\n",
        "\n",
        "  ax[0].imshow(ostu_mask, cmap=plt.cm.gray)\n",
        "  ax[0].set_title('Overlapping objects')\n",
        "  ax[1].imshow(-distance, cmap=plt.cm.gray)\n",
        "  ax[1].set_title('Distances')\n",
        "  ax[2].imshow(labels, cmap=plt.cm.nipy_spectral)\n",
        "  ax[2].set_title('Separated objects')\n",
        "  ax[3].imshow(image)\n",
        "  ax[3].set_title(ax[3].set_title(image_file.split(\"/\")[-1]))\n",
        "\n",
        "\n",
        "  for a in ax:\n",
        "      a.set_axis_off()\n",
        "\n",
        "  fig.tight_layout()\n",
        "  plt.show()\n",
        "\n",
        "  # # Display the separated objects\n",
        "  fig, axes = plt.subplots(ncols=len(separated_objects), figsize=(len(separated_objects)*3, 3), sharex=True, sharey=True)\n",
        "  for i, ax in enumerate(axes):\n",
        "      ax.imshow(separated_objects[i], cmap=plt.cm.gray)\n",
        "      ax.set_title(f'Object {i+1}')\n",
        "\n",
        "  for ax in axes:\n",
        "      ax.set_axis_off()\n",
        "\n",
        "  fig.tight_layout()\n",
        "  plt.show()\n",
        "\n",
        "    # Get the region properties\n",
        "  props = regionprops(labels)\n",
        "  len(props)\n",
        "\n",
        "  # Calculate total area and number of objects\n",
        "  total_area = np.sum([prop.area for prop in props])\n",
        "  num_objects = len(props)\n",
        "\n",
        "  # Initialize variables for average orientation, perimeter, and eccentricity\n",
        "  avg_orientation = 0\n",
        "  avg_perimeter = 0\n",
        "  avg_eccentricity = 0\n",
        "\n",
        "  # Print the area, percentage area, and centroid coordinates of each region\n",
        "  for i, prop in enumerate(props):\n",
        "      area = prop.area\n",
        "      percent_area = area / total_area * 100\n",
        "      centroid = prop.centroid\n",
        "      orientation = prop.orientation\n",
        "      perimeter = prop.perimeter\n",
        "      eccentricity = prop.eccentricity\n",
        "      # Add orientation, perimeter, and eccentricity to the running totals\n",
        "      avg_orientation += orientation\n",
        "      avg_perimeter += perimeter\n",
        "      avg_eccentricity += eccentricity\n",
        "\n",
        "      # print(f\"Object {i+1}: Area={area}, Percent Area={percent_area:.2f}%, Centroid={centroid}, Orientation={orientation:.2f}, Perimeter={perimeter:.2f}, Eccentricity={eccentricity:.2f}\")\n",
        "\n",
        "\n",
        "  # Calculate average area and total percentage area\n",
        "  avg_area2 = total_area / num_objects\n",
        "  avg_area1 = avg_area2 * (physical_size**2)\n",
        "  avg_area =\"{:.2f}\".format(avg_area1)\n",
        "  total_percent_area1 = total_area / (ostu_mask.shape[0] * ostu_mask.shape[1]) * 100\n",
        "  total_percent_area =\"{:.2f}\".format(total_percent_area1)\n",
        "\n",
        "  # Calculate average orientation, perimeter, and eccentricity\n",
        "  avg_orientation /= num_objects\n",
        "  avg_perimeter /= num_objects\n",
        "  avg_eccentricity /= num_objects\n",
        "  avg_perimeter2 = avg_perimeter * physical_size\n",
        "  avg_orientation1 =\"{:.2f}\".format(avg_orientation)\n",
        "  avg_perimeter1 =\"{:.2f}\".format(avg_perimeter2)\n",
        "  avg_eccentricity1 =\"{:.2f}\".format(avg_eccentricity)\n",
        "\n",
        "\n",
        "  # Print the total number of objects, average area, and total percentage area\n",
        "  print(f\"Total number of objects: {num_objects}\")\n",
        "  print(f\"Average area: {avg_area}\")\n",
        "  print(f\"Total percentage area: {total_percent_area}%\")\n",
        "  print(f\"Average orientation: {avg_orientation1}\")\n",
        "  print(f\"Average perimeter: {avg_perimeter1}\")\n",
        "  print(f\"Average eccentricity: {avg_eccentricity1}\")\n",
        "\n",
        "\n",
        "\n",
        "  data[\"Image\"].append(image_file.split(\"/\")[-1])\n",
        "  data[\"Average Area\"].append(avg_area)\n",
        "  data[\"Number of Objects\"].append(num_objects)\n",
        "  data[\"Total Percentage Area\"].append(total_percent_area)\n",
        "  data[\"Average Perimeter\"].append(avg_perimeter1)\n",
        "  data[\"Average Eccentricity\"].append(avg_eccentricity1)\n",
        "  data[\"Average Orientation\"].append(avg_orientation1)\n",
        "\n",
        "   # Load the image\n",
        "  img = ostu_mask\n",
        "\n",
        "  # Perform watershed segmentation to separate the objects\n",
        "  distance = ndi.distance_transform_edt(img)\n",
        "  local_maxi = peak_local_max(distance, indices=False, footprint=np.ones((3, 3)), labels=img)\n",
        "  markers = ndi.label(local_maxi)[0]\n",
        "  labels = watershed(-distance, markers, mask=img)\n",
        "\n",
        "  # Measure the properties of the labeled objects\n",
        "  props = measure.regionprops(labels)\n",
        "\n",
        "  # Define the subtypes\n",
        "  subtypes = {\n",
        "      'loop': [],\n",
        "      'globule': [],\n",
        "      'lumps': [],\n",
        "      'short': [],\n",
        "      'medium_length': [],\n",
        "      'long': [],\n",
        "      'branch': []\n",
        "  }\n",
        "\n",
        "  # Initialize subtype areas\n",
        "  loop_area = 0\n",
        "  globule_area = 0\n",
        "  lumps_area = 0\n",
        "  short_area = 0\n",
        "  medium_length_area = 0\n",
        "  long_area = 0\n",
        "  branch_area = 0\n",
        "\n",
        "  # Calculate the properties of each object and classify it into a subtype\n",
        "  for prop in props:\n",
        "      # Calculate the axial ratio of the object\n",
        "      if prop.minor_axis_length == 0:\n",
        "          axial_ratio = 1\n",
        "      else:\n",
        "          axial_ratio = prop.major_axis_length / prop.minor_axis_length\n",
        "\n",
        "      # Calculate the number of holes in the object\n",
        "      objhole = len(measure.regionprops(np.logical_xor(prop.image.astype(np.uint8), prop.convex_image.astype(np.uint8)).astype(np.int))[1:])\n",
        "\n",
        "      # Calculate the area of the object\n",
        "      area = prop.area\n",
        "\n",
        "      # Get the skeletonized image of the object\n",
        "      skel = skeletonize(prop.image)\n",
        "\n",
        "      # Calculate the maximum pixel length of the skeletonized object\n",
        "      BrMaxLength = cv2.arcLength(max(cv2.findContours(skel.astype(np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)[0], key=cv2.contourArea), closed=True)\n",
        "\n",
        "      # Calculate the number of skeletal branches of the Golgi-derived membrane structures\n",
        "      branches = cv2.connectedComponents(skel.astype(np.uint8))[0] - 1\n",
        "      BrNum = max(branches, 0)\n",
        "\n",
        "      # Classify the object into a subtype\n",
        "      if objhole == 1:\n",
        "          subtypes['loop'].append(prop)\n",
        "          loop_area += area\n",
        "      elif area <= 30 and objhole == 0:\n",
        "          subtypes['globule'].append(prop)\n",
        "          globule_area += area\n",
        "      elif area > 30 and axial_ratio <= 2 and objhole == 0:\n",
        "          subtypes['lumps'].append(prop)\n",
        "          lumps_area += area\n",
        "      elif area > 30 and axial_ratio > 2 and objhole == 0 and BrNum == 1 and BrMaxLength <= 35:\n",
        "          subtypes['short'].append(prop)\n",
        "          short_area += area\n",
        "      elif area > 30 and axial_ratio > 2 and objhole == 0 and BrNum == 1 and 35 < BrMaxLength < 70:\n",
        "          subtypes['medium_length'].append(prop)\n",
        "          medium_length_area += area\n",
        "      elif area > 30 and axial_ratio > 2 and objhole == 0 and BrNum > 1:\n",
        "          subtypes['branch'].append(prop)\n",
        "          branch_area += area\n",
        "\n",
        "  # Calculate the total area of each subtype in um^2\n",
        "  loop_area_um = loop_area * physical_size * physical_size\n",
        "  globule_area_um = globule_area * physical_size * physical_size\n",
        "  lumps_area_um = lumps_area * physical_size * physical_size\n",
        "  short_area_um = short_area * physical_size * physical_size\n",
        "  medium_length_area_um = medium_length_area * physical_size * physical_size\n",
        "  long_area_um = long_area * physical_size * physical_size\n",
        "  branch_area_um = branch_area * physical_size * physical_size\n",
        "\n",
        "  # Calculate the total area of all subtypes\n",
        "  total_area = loop_area + globule_area + lumps_area + short_area + medium_length_area + long_area + branch_area\n",
        "  total_area_um = total_area * physical_size * physical_size\n",
        "\n",
        "  # Calculate the percentage of each subtype\n",
        "  loop_percent = loop_area / total_area * 100\n",
        "  globule_percent = globule_area / total_area * 100\n",
        "  lumps_percent = lumps_area / total_area * 100\n",
        "  short_percent = short_area / total_area * 100\n",
        "  medium_length_percent = medium_length_area / total_area * 100\n",
        "  long_percent = long_area / total_area * 100\n",
        "  branch_percent = branch_area / total_area * 100\n",
        "\n",
        "  loop_area_um1 =\"{:.2f}\".format(loop_area_um)\n",
        "  loop_len = len(subtypes['loop'])\n",
        "  loop_percent1 = \"{:.2f}\".format(loop_percent)\n",
        "\n",
        "  globule_area_um1 = \"{:.2f}\".format(globule_area_um)\n",
        "  globule_len =len(subtypes['globule'])\n",
        "  globule_percent1 = \"{:.2f}\".format(globule_percent)\n",
        "\n",
        "  lumps_area_um1 =\"{:.2f}\".format(lumps_area_um)\n",
        "  lumps_len =len(subtypes['lumps'])\n",
        "  lumps_percent1 = \"{:.2f}\".format(lumps_percent)\n",
        "\n",
        "  short_area_um1 =\"{:.2f}\".format(short_area_um)\n",
        "  short_len =len(subtypes['short'])\n",
        "  short_percent1 = \"{:.2f}\".format(short_percent)\n",
        "\n",
        "  medium_length_area_um1 =\"{:.2f}\".format(medium_length_area_um)\n",
        "  medium_length_len =len(subtypes['medium_length'])\n",
        "  medium_length_percent1 = \"{:.2f}\".format(medium_length_percent)\n",
        "\n",
        "  long_area_um1 =\"{:.2f}\".format(long_area_um)\n",
        "  long_len =len(subtypes['long'])\n",
        "  long_percent1 = \"{:.2f}\".format(long_percent)\n",
        "\n",
        "  branch_area_um1 =\"{:.2f}\".format(branch_area_um)\n",
        "  branch_len =len(subtypes['branch'])\n",
        "  branch_percent1 = \"{:.2f}\".format(branch_percent)\n",
        "\n",
        "  total_area_um1 = \"{:.2f}\".format(total_area_um)\n",
        "\n",
        "\n",
        "\n",
        "  # Print the results\n",
        "  print(f\"Loop subtype: {len(subtypes['loop'])} objects, {loop_percent:.2f}% of total area ({loop_area_um:.2f} um^2)\")\n",
        "  print(f\"Globule subtype: {len(subtypes['globule'])} objects, {globule_percent:.2f}% of total area ({globule_area_um:.2f} um^2)\")\n",
        "  print(f\"Lumps subtype: {len(subtypes['lumps'])} objects, {lumps_percent:.2f}% of total area ({lumps_area_um:.2f} um^2)\")\n",
        "  print(f\"Short subtype: {len(subtypes['short'])} objects, {short_percent:.2f}% of total area ({short_area_um:.2f} um^2)\")\n",
        "  print(f\"Medium length subtype: {len(subtypes['medium_length'])} objects, {medium_length_percent:.2f}% of total area ({medium_length_area_um:.2f} um^2)\")\n",
        "  print(f\"Long subtype: {len(subtypes['long'])} objects, {long_percent:.2f}% of total area ({long_area_um:.2f} um^2)\")\n",
        "  print(f\"Branch subtype: {len(subtypes['branch'])} objects, {branch_percent:.2f}% of total area ({branch_area_um:.2f} um^2)\")\n",
        "  print(f\"Total area: {total_area} pixels, {total_area_um:.2f} um^2\")\n",
        "\n",
        "\n",
        "  data[\"Loop area\"].append(loop_area_um1)\n",
        "  data[\"Loop subtype\"].append(loop_len)\n",
        "  data[ \"Loop percent\"].append(loop_percent1)\n",
        "\n",
        "  data[\"globule area\"].append(globule_area_um1)\n",
        "  data[\"globule subtype\"].append(globule_len)\n",
        "  data[\"globule  percent\"].append(globule_percent1)\n",
        "\n",
        "  data[\"lumps area\"].append(lumps_area_um1)\n",
        "  data[\"lumps subtype\"].append(lumps_len)\n",
        "  data[\"lumps percent\"].append(lumps_percent1)\n",
        "\n",
        "  data[\"short area\"].append(short_area_um1)\n",
        "  data[\"short subtype\"].append(short_len)\n",
        "  data[\"short percent\"].append(short_percent1)\n",
        "\n",
        "  data[\"medium_length area\"].append(medium_length_area_um1)\n",
        "  data[\"medium_length subtype\"].append(medium_length_len)\n",
        "  data[\"medium_length percent\"].append(medium_length_percent1)\n",
        "\n",
        "  data[\"long area\"].append(long_area_um1)\n",
        "  data[\"long subtype\"].append(long_len)\n",
        "  data[\"long percent\"].append(long_percent1)\n",
        "\n",
        "  data[\"branch area\"].append(branch_area_um1)\n",
        "  data[\"branch subtype\"].append(branch_len)\n",
        "  data[\"branch  percent\"].append(branch_percent1)\n",
        "\n",
        "  data[\"Total subtypes area\"].append(total_area_um1)\n",
        "\n"
      ],
      "metadata": {
        "id": "sesSWAiBeqqm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split(img,n,image_file):\n",
        "# n is mo. of cells in image\n",
        "  green= img[:,:,1]\n",
        "  threshold = green.max() * 0.1\n",
        "  mask1 = green > threshold\n",
        "  image1 = mask1\n",
        "  # Threshold image to create binary mask\n",
        "  mask2 = image1 > 0.5\n",
        "\n",
        "\n",
        "  # Label connected regions in binary mask\n",
        "  label_image2 = measure.label(mask2)\n",
        "\n",
        "  # Get region properties\n",
        "  props = measure.regionprops(label_image2)\n",
        "\n",
        "  # Find indices of n regions with largest area\n",
        "\n",
        "  max_areas = [0] * n\n",
        "  max_indices = [0] * n\n",
        "  for i, prop in enumerate(props):\n",
        "      area = prop.area\n",
        "      for j in range(n):\n",
        "          if area > max_areas[j]:\n",
        "              max_areas[j+1:n] = max_areas[j:n-1]\n",
        "              max_indices[j+1:n] = max_indices[j:n-1]\n",
        "              max_areas[j] = area\n",
        "              max_indices[j] = i\n",
        "              break\n",
        "\n",
        "  # Create output images with n largest regions as one and all others as zero\n",
        "  output_images = [np.zeros_like(image1) for _ in range(n)]\n",
        "  for i in range(n):\n",
        "      output_images[i][label_image2 == max_indices[i]+1] = 1\n",
        "\n",
        "  # Display output images\n",
        "\n",
        "  # Create a subplot with 1 row and n+1 columns\n",
        "  fig, axes = plt.subplots(nrows=1, ncols=n+1, figsize=(10, 5))\n",
        "\n",
        "  # Display the original image in the first column\n",
        "  axes[0].imshow(img)\n",
        "  axes[0].set_title('Original')\n",
        "\n",
        "  # Display the output images in the remaining columns\n",
        "  for i in range(n):\n",
        "      axes[i+1].imshow(output_images[i])\n",
        "      axes[i+1].set_title(f'Region {i+1}')\n",
        "\n",
        "  # Show the plot\n",
        "  plt.show()\n",
        "\n",
        "  for i in np.arange(n):\n",
        "    output_images[i] = output_images[i]*green\n",
        "\n",
        "\n",
        "\n",
        "  for i in output_images:\n",
        "    i = color.gray2rgb(i)\n",
        "    func(i,image_file)\n",
        "\n",
        "  return output_images\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "RnRsULnieyp7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "  # Create a dictionary to store the data\n",
        "data = {\"Image\": [], \"Number of Objects\": [], \"Average Area\": [], \"Average Perimeter\": [], \"Average Eccentricity\": [], \"Average Orientation\": [], \"Total Percentage Area\": [], \"Loop area\" : [], \"Loop subtype\" : [], \"Loop percent\" : [], \"globule area\" : [], \"globule subtype\" : [], \"globule  percent\" : [], \"lumps area\" : [], \"lumps subtype\" : [], \"lumps percent\" : [], \"short area\" : [], \"short subtype\" : [], \"short percent\" : [], \"medium_length area\" : [], \"medium_length subtype\" : [], \"medium_length percent\" : [], \"long area\" : [], \"long subtype\": [], \"long percent\" : [], \"branch area\" : [],  \"branch subtype\": [], \"branch  percent\" : [], \"Total subtypes area\" : [] }\n"
      ],
      "metadata": {
        "id": "zVJ6nCThetVA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_files = glob.glob(os.path.join('path', '*.tif'))"
      ],
      "metadata": {
        "id": "1ZbZABF3e44N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Process each image in the directory\n",
        "for image_file in image_files:\n",
        "  r = imread(image_file)\n",
        "  split(r,2,image_file) #taken n=2 as example\n",
        "  # Convert the dictionary to a pandas DataFrame\n",
        "df = pd.DataFrame(data)"
      ],
      "metadata": {
        "id": "FYyWexope7Eg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify the path to the desired folder in your Google Drive\n",
        "path = \"path\"\n",
        "\n",
        "\n",
        "# Check if the folder exists, and create it if it doesn't\n",
        "if not os.path.exists(path):\n",
        "    os.makedirs(path)\n",
        "\n",
        "# Save the DataFrame to an Excel sheet in the specified folder\n",
        "filename = \"result.xlsx\"\n",
        "filepath = os.path.join(path, filename)\n",
        "df.to_excel(filepath, index=False)"
      ],
      "metadata": {
        "id": "zUzz7YSKgNeO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "PjTPUiPzqKB_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Maximum Dispersion Distance\n",
        "def max (img,image_file):\n",
        "\n",
        "  physical_size = 0.16049  #in um\n",
        "  # Print result\n",
        "  print(f\"Physical dimensions: {width_um:.2f} x {height_um:.2f} µm\")\n",
        "  physical_size = width_um / img.shape[0]    # assuming square pixels\n",
        "  print(f\"Physical size of pixel: {physical_size}\")\n",
        "  print(f\"Type of image: {img.dtype}\")\n",
        "\n",
        "  # Converting to gray-scale image\n",
        "  image = img[:,:,1]\n",
        "\n",
        "\n",
        "  image_background_mean = stack.remove_background_mean(image, kernel_shape=\"square\", kernel_size=50)\n",
        "\n",
        "\n",
        "    # Apply Gaussian filter to theimage to correct the background\n",
        "  bkg_correction = filters.gaussian(image_background_mean, sigma=221)\n",
        "\n",
        "  # Divide the image by the background correction and normalize\n",
        "  corrected_cell = np.divide(image_background_mean, bkg_correction/bkg_correction.max())\n",
        "\n",
        "  # Apply Otsu threshold to the corrected image to segment foreground and background\n",
        "  threshold = filters.threshold_otsu(corrected_cell)\n",
        "  ostu_mask = corrected_cell > threshold\n",
        "\n",
        "\n",
        "\n",
        "  # Watershed segmentation\n",
        "\n",
        "  # Now we want to separate the objects in image\n",
        "  # Generate the markers as local maxima of the distance to the background\n",
        "  distance = ndi.distance_transform_edt(ostu_mask)\n",
        "  coords = peak_local_max(distance, footprint=np.ones((3, 3)), labels=ostu_mask)\n",
        "  mask = np.zeros(distance.shape, dtype=bool)\n",
        "  mask[tuple(coords.T)] = True\n",
        "  markers, _ = ndi.label(mask)\n",
        "  labels = watershed(-distance, markers, mask=ostu_mask)\n",
        "\n",
        "  # Find the bounding boxes of each labeled object\n",
        "  objects = ndi.find_objects(labels)\n",
        "\n",
        "  # Extract each object as a separate array using NumPy indexing\n",
        "  separated_objects = [ostu_mask[obj] for obj in objects]\n",
        "\n",
        "\n",
        "    # Get the region properties\n",
        "  props = regionprops(labels)\n",
        "  len(props)\n",
        "\n",
        "  # Calculate total area and number of objects\n",
        "  total_area = np.sum([prop.area for prop in props])\n",
        "  num_objects = len(props)\n",
        "\n",
        "  # Initialize variables for average orientation, perimeter, and eccentricity\n",
        "  avg_orientation = 0\n",
        "  avg_perimeter = 0\n",
        "  avg_eccentricity = 0\n",
        "\n",
        "  # Print the area, percentage area, and centroid coordinates of each region\n",
        "  for i, prop in enumerate(props):\n",
        "      area = prop.area\n",
        "      percent_area = area / total_area * 100\n",
        "      centroid = prop.centroid\n",
        "      orientation = prop.orientation\n",
        "      perimeter = prop.perimeter\n",
        "      eccentricity = prop.eccentricity\n",
        "      # Add orientation, perimeter, and eccentricity to the running totals\n",
        "      avg_orientation += orientation\n",
        "      avg_perimeter += perimeter\n",
        "      avg_eccentricity += eccentricity\n",
        "\n",
        "      # print(f\"Object {i+1}: Area={area}, Percent Area={percent_area:.2f}%, Centroid={centroid}, Orientation={orientation:.2f}, Perimeter={perimeter:.2f}, Eccentricity={eccentricity:.2f}\")\n",
        "\n",
        "\n",
        "\n",
        "  # Initialize a list to store the centroid coordinates of each object\n",
        "  centroids = []\n",
        "\n",
        "  # Print the centroid coordinates of each object\n",
        "\n",
        "  for i, prop in enumerate(props):\n",
        "      centroid = prop.centroid\n",
        "      centroids.append(centroid)\n",
        "\n",
        "  # Masking\n",
        "  cell = img[:,:,1]\n",
        "  image_bool = cell > 25\n",
        "  # images = [cell,image_bool]\n",
        "  # titles = [\"image\",\"masked image\"]\n",
        "  # plot.plot_images(images, rescale=True, titles=titles)\n",
        "\n",
        "  # GETTING NUCELUS\n",
        "  # Invert the image\n",
        "  inverted_image = 1 - image_bool\n",
        "\n",
        "  # Apply clear boundary to make the background zero\n",
        "  cleared_image = segmentation.clear_border(inverted_image)\n",
        "\n",
        "  # fill holes\n",
        "  filled_image = ndimage.binary_fill_holes(cleared_image)\n",
        "\n",
        "  # FInding largest object\n",
        "    # Threshold image to create binary mask\n",
        "  binary_mask = filled_image > 0.5\n",
        "\n",
        "  # Label connected regions in binary mask\n",
        "  label_image = measure.label(binary_mask)\n",
        "\n",
        "  # Get region properties\n",
        "  props = measure.regionprops(label_image)\n",
        "\n",
        "  # Find index of largest region\n",
        "  max_area = 0\n",
        "  max_index = 0\n",
        "  for i, prop in enumerate(props):\n",
        "      if prop.area > max_area:\n",
        "          max_area = prop.area\n",
        "          max_index = i\n",
        "\n",
        "  # Create output image with largest region as one and all others as zero\n",
        "  output_image = np.zeros_like(cell)\n",
        "  output_image[label_image == max_index+1] = 1\n",
        "\n",
        "  # # Display output image\n",
        "  # io.imshow(output_image, cmap='gray')\n",
        "  # io.show()\n",
        "\n",
        "  # converting bool to 8 bit\n",
        "  nucleus = output_image.astype(np.uint8)\n",
        "  # plt.imshow(nucleus)\n",
        "\n",
        "  # FINDING NUCLEUS CENTER\n",
        "    # Find the indices of all non-zero elements in the image\n",
        "  indices = np.nonzero(nucleus)\n",
        "\n",
        "  # Calculate the centroid\n",
        "  centroid = np.mean(indices, axis=1)\n",
        "\n",
        "  # Mark the centroid on the image\n",
        "  nucleus[int(centroid[0]), int(centroid[1])] = 2\n",
        "\n",
        "  # Plot the image with centroid marked in red\n",
        "  fig, ax = plt.subplots()\n",
        "  ax.imshow(nucleus, cmap='gray')\n",
        "  ax.scatter(centroid[1], centroid[0], c='r')\n",
        "  ax.set_axis_off()\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "\n",
        "  Mark the centroid on the image\n",
        "  ostu_mask[int(centroid[0]), int(centroid[1])] = 2\n",
        "\n",
        "  # Plot the image with centroid marked in red\n",
        "  fig, ax = plt.subplots()\n",
        "  ax.imshow(ostu_mask, cmap='gray')\n",
        "  ax.scatter(centroid[0], centroid[1], c='r')\n",
        "  ax.set_axis_off()\n",
        "  plt.show()\n",
        "\n",
        "  # FINDING FARTHEST PUNCTA\n",
        "  x=[]\n",
        "  y=[]\n",
        "  for i in range(ostu_mask.shape[0]):\n",
        "    for j in range(ostu_mask.shape[0]):\n",
        "      if ostu_mask[i,j]==1:\n",
        "        x.append(i)\n",
        "        y.append(j)\n",
        "  x1=np.array(x)\n",
        "  y1=np.array(y)\n",
        "  i=0\n",
        "  centre=[centroid[0],centroid[1]]\n",
        "  ListDistance=[]\n",
        "  for _ in range(int(x1.shape[0]/2)):\n",
        "    point=[x1[i],y1[i]]\n",
        "    distance= math.dist(point,centre)\n",
        "\n",
        "    ListDistance.append(distance)\n",
        "    i+=1\n",
        "  max = ListDistance.index(np.array(ListDistance).max())\n",
        "\n",
        "  max_x = x1[max]\n",
        "  max_y = y1[max]\n",
        "  max_dispersion_length_pixel = ListDistance[max]\n",
        "    # Convert max dispersion length from pixel units to micrometers\n",
        "  pixel_size = physical_size # µm/pixel\n",
        "  max_dispersion1 = max_dispersion_length_pixel * pixel_size\n",
        "  max_dispersion =\"{:.2f}\".format(max_dispersion1)\n",
        "\n",
        "  # print(f\"{max_dispersion_length_pixel} pixels is maximum dispersion length & coordinates are {max_x}, {max_y}\")\n",
        "  # print(f\"{max_dispersion} µm is maximum dispersion length in micrometers\")\n",
        "\n",
        "\n",
        "  # Drawing maximum dispersion line\n",
        "  cell_image =(ostu_mask.astype(np.uint8) * 255) #converting to 8-bit\n",
        "  # Define points\n",
        "  pt1 = (max_y,max_x)\n",
        "  pt2 = (int(centroid[1]),int(centroid[0]))\n",
        "\n",
        "  # Draw line\n",
        "  color = (255) # BGR format\n",
        "  thickness = 2\n",
        "  cv2.line(cell_image, pt1, pt2, color, thickness)\n",
        "\n",
        "  # Display image\n",
        "  cv2_imshow(cell_image)\n",
        "  cv2.waitKey(0)\n",
        "  cv2.destroyAllWindows()\n",
        "    # Populate the dictionary with data\n",
        "\n",
        "  data[\"Image\"].append(image_file.split(\"/\")[-1])\n",
        "  data[\"Maximum Dispersion\"].append(max_dispersion)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "FA7eOrMzqReT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split(img,n,image_file):\n",
        "\n",
        "  green= img[:,:,1]\n",
        "  threshold = green.max() * 0.1\n",
        "  mask1 = green > threshold\n",
        "  image1 = mask1\n",
        "  # Threshold image to create binary mask\n",
        "  mask2 = image1 > 0.5\n",
        "\n",
        "\n",
        "  # Label connected regions in binary mask\n",
        "  label_image2 = measure.label(mask2)\n",
        "\n",
        "  # Get region properties\n",
        "  props = measure.regionprops(label_image2)\n",
        "\n",
        "  # Find indices of n regions with largest area\n",
        "\n",
        "  max_areas = [0] * n\n",
        "  max_indices = [0] * n\n",
        "  for i, prop in enumerate(props):\n",
        "      area = prop.area\n",
        "      for j in range(n):\n",
        "          if area > max_areas[j]:\n",
        "              max_areas[j+1:n] = max_areas[j:n-1]\n",
        "              max_indices[j+1:n] = max_indices[j:n-1]\n",
        "              max_areas[j] = area\n",
        "              max_indices[j] = i\n",
        "              break\n",
        "\n",
        "  # Create output images with n largest regions as one and all others as zero\n",
        "  output_images = [np.zeros_like(image1) for _ in range(n)]\n",
        "  for i in range(n):\n",
        "      output_images[i][label_image2 == max_indices[i]+1] = 1\n",
        "\n",
        "  # Display output images\n",
        "\n",
        "  # Create a subplot with 1 row and n+1 columns\n",
        "  fig, axes = plt.subplots(nrows=1, ncols=n+1, figsize=(10, 5))\n",
        "\n",
        "  # Display the original image in the first column\n",
        "  axes[0].imshow(img)\n",
        "  axes[0].set_title(image_file.split(\"/\")[-1])\n",
        "\n",
        "  # Display the output images in the remaining columns\n",
        "  for i in range(n):\n",
        "      axes[i+1].imshow(output_images[i])\n",
        "      axes[i+1].set_title(f'I {i+1}')\n",
        "\n",
        "  # Show the plot\n",
        "  plt.show()\n",
        "\n",
        "  for i in np.arange(n):\n",
        "    output_images[i] = output_images[i]*green\n",
        "\n",
        "\n",
        "\n",
        "  for i in output_images:\n",
        "    i = color.gray2rgb(i)\n",
        "    max(i,image_file)\n",
        "\n",
        "  return output_images"
      ],
      "metadata": {
        "id": "scxTs9DJrZxv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = {\"Image\": [], \"Maximum Dispersion\": [] }"
      ],
      "metadata": {
        "id": "w8qnuOthrbNK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Process each image in the directory\n",
        "for image_file in image_files:\n",
        "  try:\n",
        "    r = imread(image_file)\n",
        "    split(r,2,image_file) #taken n=2 as example\n",
        "    pass\n",
        "  except Exception as e:\n",
        "        print(f\"Error in iteration {image_file}: {e}\")\n",
        "        continue\n",
        "  # Convert the dictionary to a pandas DataFrame\n",
        "df = pd.DataFrame(data)"
      ],
      "metadata": {
        "id": "v4Iqa5qtrwz1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify the path to the desired folder in your Google Drive\n",
        "path = \"path\"\n",
        "\n",
        "\n",
        "# Check if the folder exists, and create it if it doesn't\n",
        "if not os.path.exists(path):\n",
        "    os.makedirs(path)\n",
        "\n",
        "# Save the DataFrame to an Excel sheet in the specified folder\n",
        "filename = \"result.xlsx\"\n",
        "filepath = os.path.join(path, filename)\n",
        "df.to_excel(filepath, index=False)"
      ],
      "metadata": {
        "id": "AaNJQZfCrxLB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BUdkB1JLvFLS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}